{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "import random\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttype = torch.FloatTensor         # For CPU\n",
    "# ttype = torch.cuda.FloatTensor  # For GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagedata = np.zeros((0, 1024 * 3))\n",
    "\n",
    "for numfile in range(4):\n",
    "    with open('cifar-10-batches-py/data_batch_' + str(numfile + 1), 'rb') as fo:\n",
    "        imagedict = pickle.load(fo, encoding='bytes')  # Python 3\n",
    "    imagedata = np.concatenate((imagedata, imagedict[b'data']), axis=0)\n",
    "\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaultParams = {\n",
    "    'nbpatterns': 3,  # number of images per episode\n",
    "    'nbprescycles': 3,  # number of presentations for each image\n",
    "    'prestime': 20,  # number of time steps for each image presentation\n",
    "    'prestimetest': 3,  # number of time steps for the test (degraded) image\n",
    "    'interpresdelay': 2,  # number of time steps (with zero input) between two presentations\n",
    "    'patternsize': 1024,  # size of the images (32 x 32 = 1024)\n",
    "    'nbiter': 100000,  # number of episodes\n",
    "\n",
    "    # when contiguousperturbation is False (which it shouldn't be),\n",
    "    # probability of zeroing each pixel in the test image\n",
    "    'probadegrade': .5,\n",
    "\n",
    "    'lr': 1e-4,  # Adam learning rate\n",
    "    'print_every': 10,  # how often to print statistics and save files\n",
    "    'homogenous': 0,  # whether alpha should be shared across connections\n",
    "    'rngseed': 0  # random seed\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate inputs and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inputs_and_target(params, contiguousperturbation=True):\n",
    "    input_tensor = np.zeros((params['nbsteps'], 1, params['nbneur']))  # inputTensor, initially in numpy format...\n",
    "\n",
    "    # Create the random patterns to be memorized in an episode\n",
    "    # Floating-point, graded patterns, zero-mean\n",
    "    patterns = []\n",
    "    for nump in range(params['nbpatterns']):\n",
    "        numpic = np.random.randint(imagedata.shape[0])\n",
    "        p = imagedata[numpic].reshape((3, 1024)).sum(0).astype(float)\n",
    "        p = p[:params['patternsize']]\n",
    "        p = p - np.mean(p)\n",
    "        p = p / (1e-8 + np.max(np.abs(p)))\n",
    "        # p = (np.random.randint(2, size=params['patternsize']) - .5) *2   # Binary patterns\n",
    "        patterns.append(p)\n",
    "\n",
    "    # Now 'patterns' contains the NBPATTERNS patterns to be memorized in this episode - in numpy format\n",
    "    # Creating the test pattern, partially zero'ed out, that the network will have to complete\n",
    "    testpattern = random.choice(patterns).copy()\n",
    "    preservedbits = np.ones(params['patternsize'])\n",
    "\n",
    "    # Contiguous perturbation = one contiguous half of the image is zeroed out. Default (see above).\n",
    "    if contiguousperturbation:\n",
    "        preservedbits[int(params['patternsize'] / 2):] = 0\n",
    "        if np.random.rand() < .5:\n",
    "            preservedbits = 1 - preservedbits\n",
    "\n",
    "    # Otherwise, randomly zero out individual pixels. Because natural images are highly\n",
    "    # autocorrelated, a trivial approximate solution is to take the average of nearby pixels.\n",
    "    else:\n",
    "        preservedbits[:int(params['probadegrade'] * params['patternsize'])] = 0\n",
    "        np.random.shuffle(preservedbits)\n",
    "    degradedtestpattern = testpattern * preservedbits\n",
    "\n",
    "    # Inserting the inputs in the input tensor at the proper places\n",
    "    for nc in range(params['nbprescycles']):\n",
    "        np.random.shuffle(patterns)\n",
    "        for ii in range(params['nbpatterns']):\n",
    "            for nn in range(params['prestime']):\n",
    "                numi = nc * (params['nbpatterns'] * (params['prestime'] + params['interpresdelay'])) + ii * (params['prestime'] + params['interpresdelay']) + nn\n",
    "                input_tensor[numi][0][:params['patternsize']] = patterns[ii][:]\n",
    "\n",
    "    for nn in range(params['prestimetest']):\n",
    "        input_tensor[-params['prestimetest'] + nn][0][:params['patternsize']] = degradedtestpattern[:]\n",
    "\n",
    "    for nn in range(params['nbsteps']):\n",
    "        input_tensor[nn][0][-1] = 1.0  # Bias neuron is forced to 1\n",
    "        # input_tensor[nn] *= params['inputboost']       # Strengthen inputs\n",
    "\n",
    "    input_tensor = torch.from_numpy(input_tensor).type(ttype)  # Convert from numpy to Tensor\n",
    "    target = torch.from_numpy(testpattern).type(ttype)\n",
    "\n",
    "    return input_tensor, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        # Notice that the vectors are row vectors, and the matrices are transposed\n",
    "        # wrt the comp neuro order, following deep learning / pytorch conventions\n",
    "        # Each *column* of w targets a single output neuron\n",
    "        self.w = Variable(.01 * torch.randn(params['nbneur'], params['nbneur']).type(ttype), requires_grad=True)\n",
    "\n",
    "        if params['homogenous'] == 1:\n",
    "            # plasticity coefficients: homogenous/shared across connections\n",
    "            self.alpha = Variable(.01 * torch.ones(1).type(ttype), requires_grad=True)\n",
    "        else:\n",
    "            # plasticity coefficients: independent\n",
    "            self.alpha = Variable(.01 * torch.randn(params['nbneur'], params['nbneur']).type(ttype),\n",
    "                                  requires_grad=True)\n",
    "\n",
    "        # \"learning rate\" of plasticity, shared across all connections\n",
    "        self.eta = Variable(.01 * torch.ones(1).type(ttype), requires_grad=True)\n",
    "        self.params = params\n",
    "\n",
    "    def forward(self, input, yin, hebb):\n",
    "        # Inputs are fed by clamping the output of cells that receive\n",
    "        # input at the input value, like in standard Hopfield networks\n",
    "        # clamps = torch.zeros(1, self.params['nbneur'])\n",
    "        clamps = np.zeros(self.params['nbneur'])\n",
    "        zz = torch.nonzero(input.data[0].cpu()).numpy().squeeze()\n",
    "\n",
    "        clamps[zz] = 1\n",
    "\n",
    "        clamps = Variable(torch.from_numpy(clamps).type(ttype), requires_grad=False).float()\n",
    "        yout = F.tanh(yin.mm(self.w + torch.mul(self.alpha, hebb))) * (1 - clamps) + input * clamps\n",
    "\n",
    "        # bmm used to implement outer product\n",
    "        hebb = (1 - self.eta) * hebb + self.eta * torch.bmm(yin.unsqueeze(2), yout.unsqueeze(1))[0]\n",
    "        return yout, hebb\n",
    "\n",
    "    def initialZeroState(self):\n",
    "        return Variable(torch.zeros(1, self.params['nbneur']).type(ttype))\n",
    "\n",
    "    def initialZeroHebb(self):\n",
    "        return Variable(torch.zeros(self.params['nbneur'], self.params['nbneur']).type(ttype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Passed params:  {'prestime': 20, 'nbiter': 100000, 'nbpatterns': 3, 'probadegrade': 0.5, 'lr': 0.0001, 'print_every': 10, 'homogenous': 0, 'rngseed': 0, 'patternsize': 1024, 'prestimetest': 3, 'interpresdelay': 2, 'nbprescycles': 3}\n",
      "uname_result(system='Linux', node='joan-PC', release='4.15.0-36-generic', version='#39~16.04.1-Ubuntu SMP Tue Sep 25 08:59:23 UTC 2018', machine='x86_64', processor='x86_64')\n",
      "Setting random seeds\n",
      "Initializing network\n",
      "Initializing optimizer\n",
      "Starting episodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joan/.virtualenvs/tensorflow-cpu/lib/python3.5/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/joan/.virtualenvs/tensorflow-cpu/lib/python3.5/site-packages/ipykernel_launcher.py:57: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "params = {}\n",
    "params.update(defaultParams)\n",
    "\n",
    "print(\"Passed params: \", params)\n",
    "print(platform.uname())\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Total number of steps per episode\n",
    "params['nbsteps'] = params['nbprescycles'] * ((params['prestime'] + params['interpresdelay']) * params['nbpatterns']) + params['prestimetest']\n",
    "params['nbneur'] = params['patternsize'] + 1\n",
    "\n",
    "# Turning the parameters into a nice suffix for filenames; rngseed always appears last\n",
    "suffix = \"images_\" + \"\".join([str(x) + \"_\"\n",
    "                              if pair[0] is not 'nbneur'\n",
    "                                 and pair[0] is not 'nbsteps'\n",
    "                                 and pair[0] is not 'print_every'\n",
    "                                 and pair[0] is not 'rngseed'\n",
    "                              else ''\n",
    "                              for pair in zip(params.keys(), params.values())\n",
    "                              for x in pair])[:-1] + '_rngseed_' + str(params['rngseed'])\n",
    "\n",
    "# Initialize random seeds (first two redundant?)\n",
    "print(\"Setting random seeds\")\n",
    "np.random.seed(params['rngseed'])\n",
    "random.seed(params['rngseed'])\n",
    "torch.manual_seed(params['rngseed'])\n",
    "\n",
    "print(\"Initializing network\")\n",
    "net = Network(params)\n",
    "total_loss = 0.0\n",
    "\n",
    "print(\"Initializing optimizer\")\n",
    "optimizer = torch.optim.Adam([net.w, net.alpha, net.eta], lr=params['lr'])\n",
    "all_losses = []\n",
    "\n",
    "nowtime = time.time()\n",
    "print(\"Starting episodes...\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "for numiter in range(params['nbiter']):\n",
    "    y = net.initialZeroState()\n",
    "    hebb = net.initialZeroHebb()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    inputs, target = generate_inputs_and_target(params)\n",
    "\n",
    "    # Running the episode\n",
    "    for numstep in range(params['nbsteps']):\n",
    "        y, hebb = net(Variable(inputs[numstep], requires_grad=False), y, hebb)\n",
    "\n",
    "    # Computing gradients, applying optimizer\n",
    "    loss = (y[0][:params['patternsize']] - Variable(target, requires_grad=False)).pow(2).sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    lossnum = loss.data[0]\n",
    "    total_loss += lossnum\n",
    "\n",
    "    # Printing statistics, saving files\n",
    "    if (numiter + 1) % params['print_every'] == 0:\n",
    "\n",
    "        print(numiter, \"====\")\n",
    "        td = target.cpu().numpy()\n",
    "        yd = y.data.cpu().numpy()[0][:-1]\n",
    "\n",
    "        print(\"y: \", yd[:10])\n",
    "        print(\"target: \", td[:10])\n",
    "\n",
    "        absdiff = np.abs(td - yd)\n",
    "\n",
    "        print(\"Mean / median / max abs diff:\", np.mean(absdiff), np.median(absdiff), np.max(absdiff))\n",
    "        print(\"Correlation (full / sign): \", np.corrcoef(td, yd)[0][1], np.corrcoef(np.sign(td), np.sign(yd))[0][1])\n",
    "\n",
    "        previoustime = nowtime\n",
    "        nowtime = time.time()\n",
    "\n",
    "        print(\"Time spent on last\", params['print_every'], \"iters: \", nowtime - previoustime)\n",
    "\n",
    "        total_loss /= params['print_every']\n",
    "        all_losses.append(total_loss)\n",
    "\n",
    "        print(\"Mean loss over last\", params['print_every'], \"iters:\", total_loss)\n",
    "        print(\"Saving local files...\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        with open('results_' + suffix + '.dat', 'wb') as fo:\n",
    "            pickle.dump(net.w.data.cpu().numpy(), fo)\n",
    "            pickle.dump(net.alpha.data.cpu().numpy(), fo)\n",
    "            pickle.dump(net.eta.data.cpu().numpy(), fo)\n",
    "            pickle.dump(all_losses, fo)\n",
    "            pickle.dump(params, fo)\n",
    "\n",
    "        print(\"ETA:\", net.eta.data.cpu().numpy())\n",
    "        with open('loss_' + suffix + '.txt', 'w') as thefile:\n",
    "            for item in all_losses:\n",
    "                thefile.write(\"%s\\n\" % item)\n",
    "\n",
    "        sys.stdout.flush()\n",
    "        sys.stderr.flush()\n",
    "\n",
    "        total_loss = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-cpu",
   "language": "python",
   "name": "tensorflow-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
